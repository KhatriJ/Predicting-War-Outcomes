{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Conflict Outcomes Pt. 2\n",
    "by Jigar Khatri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Image(filename = '/Users/DBerl/Deskop/Conflict Datasets/Pandas_on_Slide.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comp_war_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1360, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1360 entries, 0 to 1359\n",
      "Data columns (total 32 columns):\n",
      "stateabb             1360 non-null object\n",
      "statea               1360 non-null int64\n",
      "year                 1360 non-null int64\n",
      "defensebudget        1360 non-null float64\n",
      "milsize              1360 non-null float64\n",
      "IronSteelProduct     1360 non-null float64\n",
      "EnergyUse            1360 non-null float64\n",
      "totalpop             1360 non-null float64\n",
      "urbanpop             1360 non-null float64\n",
      "CINC_Score_A         1360 non-null float64\n",
      "version_x            1360 non-null int64\n",
      "stateb               1360 non-null int64\n",
      "warstrtmnth          1360 non-null int64\n",
      "warstrtday           1360 non-null int64\n",
      "warstrtyr            1360 non-null int64\n",
      "warendmnth           1360 non-null int64\n",
      "warenday             1360 non-null int64\n",
      "warendyr             1360 non-null int64\n",
      "warolea              1360 non-null int64\n",
      "waroleb              1360 non-null int64\n",
      "wardyadrolea         1360 non-null int64\n",
      "wardyadroleb         1360 non-null int64\n",
      "outcomea             1360 non-null float64\n",
      "batdtha              1360 non-null float64\n",
      "batdthb              1360 non-null float64\n",
      "durindx              1360 non-null int64\n",
      "diff_in_days         1360 non-null int64\n",
      "diff_in_deaths       1360 non-null float64\n",
      "total_battle_dths    1360 non-null float64\n",
      "CINC_Score_B         1360 non-null float64\n",
      "version_y            1360 non-null int64\n",
      "diff_CINC            1360 non-null float64\n",
      "dtypes: float64(14), int64(17), object(1)\n",
      "memory usage: 340.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring War Role Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warrolevaluea = df[\"warolea\"].value_counts(normalize=True)\n",
    "warrolevaluea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warrolevalueb = df[\"waroleb\"].value_counts(normalize=True)\n",
    "warrolevalueb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_role_a_dummy = pd.get_dummies(df[\"warolea\"], prefix=\"Role_A\", drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "war_role_b_dummy = pd.get_dummies(df[\"waroleb\"], prefix=\"Role_B\", drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, war_role_a_dummy, war_role_b_dummy], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"warolea\", \"waroleb\", \"wardyadrolea\", \"wardyadroleb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model and Logistic Regression\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['stateabb', 'statea', 'year', 'defensebudget', 'milsize',\n",
       "       'IronSteelProduct', 'EnergyUse', 'totalpop', 'urbanpop', 'CINC_Score_A',\n",
       "       'version_x', 'stateb', 'warstrtmnth', 'warstrtday', 'warstrtyr',\n",
       "       'warendmnth', 'warenday', 'warendyr', 'outcomea', 'batdtha', 'batdthb',\n",
       "       'durindx', 'diff_in_days', 'diff_in_deaths', 'total_battle_dths',\n",
       "       'CINC_Score_B', 'version_y', 'diff_CINC', 'Role_A_2', 'Role_A_3',\n",
       "       'Role_A_4', 'Role_B_2', 'Role_B_3', 'Role_B_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"diff_in_days\",\n",
    "    \"diff_in_deaths\",\n",
    "    \"diff_CINC\",\n",
    "    \"total_battle_dths\",\n",
    "    \"Role_A_2\",\n",
    "    \"Role_A_3\",\n",
    "    \"Role_A_4\",\n",
    "    \"Role_B_2\",\n",
    "    \"Role_B_3\",\n",
    "    \"Role_B_4\",\n",
    "    \"Role_A_2\",\n",
    "    \"Role_A_3\",\n",
    "    \"Role_A_4\",\n",
    "    \"Role_B_2\",\n",
    "    \"Role_B_3\",\n",
    "    \"Role_B_4\",\n",
    "]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"outcomea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Feature Columns Chosen\n",
    "- The dataset that I have constructed examines conventional inter-state conflict in a dyadic context. The goal of my analysis is to see how well several features such as the length of the war, a state's role in a war, the differences in casualties,  and the differences in national material capabilities predict the outcome of a war (win,lose, or draw) for a state (statea).\n",
    "- I use several models to try to see how well each of them performs when it comes to predicting outcomes and which features are most important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "ax.set_xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    535\n",
       "1.0    525\n",
       "2.0    300\n",
       "Name: outcomea, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['outcomea'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(LogisticRegression(solver=\"lbfgs\"), X, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver=\"lbfgs\", n_jobs=-1)\n",
    "\n",
    "params = {\"fit_intercept\": [True, False], \"C\": [1.0, 0.1, 0.01, 0.001]}\n",
    "\n",
    "clf = GridSearchCV(logreg, params, cv=5, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    4.8s finished\n",
      "C:\\Users\\DBerl\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'fit_intercept': [True, False], 'C': [1.0, 0.1, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.442156862745098"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'fit_intercept': True}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45294117647058824"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saw a noticeable drop in accuracy following the use of k-fold cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"diff_in_days\",\n",
    "    \"diff_in_deaths\",\n",
    "    \"diff_CINC\",\n",
    "    \"total_battle_dths\",\n",
    "    \"Role_A_2\",\n",
    "    \"Role_A_3\",\n",
    "    \"Role_A_4\",\n",
    "    \"Role_B_2\",\n",
    "    \"Role_B_3\",\n",
    "    \"Role_B_4\",\n",
    "    \"Role_A_2\",\n",
    "    \"Role_A_3\",\n",
    "    \"Role_A_4\",\n",
    "    \"Role_B_2\",\n",
    "    \"Role_B_3\",\n",
    "    \"Role_B_4\",\n",
    "]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"outcomea\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_neighbors\": range(1, 100), \"weights\": [\"uniform\", \"distance\"]}\n",
    "\n",
    "clf = GridSearchCV(knn, parameters, cv=5, scoring=\"accuracy\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 198 candidates, totalling 990 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 990 out of 990 | elapsed:   30.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': range(1, 100), 'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 14, 'weights': 'distance'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8980392156862745"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DBerl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\DBerl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\DBerl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\DBerl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\DBerl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\DBerl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\DBerl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>param_weights</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011899</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.015215</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 1, 'weights': 'uniform'}</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.901478</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004041</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.021016</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 1, 'weights': 'distance'}</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.901478</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004829</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 2, 'weights': 'uniform'}</td>\n",
       "      <td>0.868293</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.891626</td>\n",
       "      <td>0.859804</td>\n",
       "      <td>0.024805</td>\n",
       "      <td>101</td>\n",
       "      <td>0.950920</td>\n",
       "      <td>0.943627</td>\n",
       "      <td>0.957108</td>\n",
       "      <td>0.947304</td>\n",
       "      <td>0.942472</td>\n",
       "      <td>0.948286</td>\n",
       "      <td>0.005315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009376</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>{'n_neighbors': 2, 'weights': 'distance'}</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.901478</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.006252</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>{'n_neighbors': 3, 'weights': 'uniform'}</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.876847</td>\n",
       "      <td>0.836275</td>\n",
       "      <td>0.027563</td>\n",
       "      <td>102</td>\n",
       "      <td>0.919018</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.925245</td>\n",
       "      <td>0.928922</td>\n",
       "      <td>0.915545</td>\n",
       "      <td>0.920099</td>\n",
       "      <td>0.006256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.011899      0.010041         0.015215        0.002703   \n",
       "1       0.004041      0.004241         0.021016        0.005015   \n",
       "2       0.004829      0.005180         0.011190        0.006745   \n",
       "3       0.009376      0.007655         0.012495        0.006248   \n",
       "4       0.003126      0.006252         0.012500        0.006250   \n",
       "\n",
       "  param_n_neighbors param_weights                                     params  \\\n",
       "0                 1       uniform   {'n_neighbors': 1, 'weights': 'uniform'}   \n",
       "1                 1      distance  {'n_neighbors': 1, 'weights': 'distance'}   \n",
       "2                 2       uniform   {'n_neighbors': 2, 'weights': 'uniform'}   \n",
       "3                 2      distance  {'n_neighbors': 2, 'weights': 'distance'}   \n",
       "4                 3       uniform   {'n_neighbors': 3, 'weights': 'uniform'}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.902439           0.921569           0.872549           0.872549   \n",
       "1           0.902439           0.921569           0.872549           0.872549   \n",
       "2           0.868293           0.872549           0.848039           0.818627   \n",
       "3           0.902439           0.921569           0.872549           0.872549   \n",
       "4           0.800000           0.828431           0.857843           0.818627   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "0           0.901478         0.894118        0.019013               11   \n",
       "1           0.901478         0.894118        0.019013               11   \n",
       "2           0.891626         0.859804        0.024805              101   \n",
       "3           0.901478         0.894118        0.019013               11   \n",
       "4           0.876847         0.836275        0.027563              102   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "0            1.000000            1.000000            1.000000   \n",
       "1            1.000000            1.000000            1.000000   \n",
       "2            0.950920            0.943627            0.957108   \n",
       "3            1.000000            1.000000            1.000000   \n",
       "4            0.919018            0.911765            0.925245   \n",
       "\n",
       "   split3_train_score  split4_train_score  mean_train_score  std_train_score  \n",
       "0            1.000000            1.000000          1.000000         0.000000  \n",
       "1            1.000000            1.000000          1.000000         0.000000  \n",
       "2            0.947304            0.942472          0.948286         0.005315  \n",
       "3            1.000000            1.000000          1.000000         0.000000  \n",
       "4            0.928922            0.915545          0.920099         0.006256  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_results = pd.DataFrame(clf.cv_results_)\n",
    "clf_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", linewidths=0.5)\n",
    "\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_norm = cm / cm.sum(axis=1)\n",
    "\n",
    "sns.heatmap(cm_norm, annot=True, linewidths=0.5, cmap=\"Blues\")\n",
    "\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    0.397059\n",
       "1.0    0.381373\n",
       "2.0    0.221569\n",
       "Name: outcomea, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_knrf = DummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='stratified')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_knrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummy_kn = dummy_knrf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37941176470588234"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_dummy_kn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from comparing the accuracy scores of the knn model with the dummy classifier, the knn model produces \n",
    "# results that are far better than if we had simply guessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfreg = RandomForestRegressor(\n",
    "    n_estimators=150, max_features=5, oob_score=True, random_state=1\n",
    ")\n",
    "rfreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    {\"feature\": feature_cols, \"importance\": rfreg.feature_importances_}\n",
    ").sort_values(by=\"importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((rfreg.oob_score_))  # Rsquared\n",
    "\n",
    "scores = cross_val_score(rfreg, X, y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "np.mean(np.sqrt(-scores))  # RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1360, 16)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfreg = RandomForestRegressor(\n",
    "    n_estimators=150, max_features=5, oob_score=True, random_state=1\n",
    ")\n",
    "rfreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "print(SelectFromModel(rfreg, threshold=\"mean\", prefit=True).transform(X_train).shape)\n",
    "print(SelectFromModel(rfreg, threshold=\"median\", prefit=True).transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_important = SelectFromModel(rfreg, threshold=\"mean\", prefit=True).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(\n",
    "    rfreg, X_important, y_test, cv=10, scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_range = list(range(10, 300, 10))\n",
    "\n",
    "RMSE_scores = []\n",
    "\n",
    "for estimator in estimator_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(estimator_range, RMSE_scores)\n",
    "\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"RMSE (lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = list(range(1, len(feature_cols) + 1))\n",
    "\n",
    "\n",
    "RMSE_scores = []\n",
    "\n",
    "for feature in feature_range:\n",
    "    rfreg = RandomForestRegressor(\n",
    "        n_estimators=150, max_features=feature, random_state=1\n",
    "    )\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(feature_range, RMSE_scores)\n",
    "\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"RMSE (lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_knrf = DummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_knrf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dummy_kn = dummy_knrf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3558823529411765"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_dummy_kn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see, the random forest model also produces much better results than the dummy classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "- There were a few things that genuinely surprised me in the analysis.\n",
    "- The poor results from the logistic regression model, despite the grid search seems to indicates that additional work needs to be done (feature scaling, normalization, etc.)\n",
    "- The random forest and knn models performed as I had expected given that they are generally regarded to be more accurate than logistic regression models.\n",
    "- Looking at the differences between the knn and the random forest model, I can't help but wonder if the knn model is overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- Zeev Maoz, Paul L. Johnson, Jasper Kaplan, Fiona Ogunkoya, and Aaron Shreve 2019. The Dyadic Militarized Interstate Disputes (MIDs) Dataset Version 3.0: Logic, Characteristics, and Comparisons to Alternative Datasets, Journal of Conflict Resolution (forthcoming).\n",
    "\n",
    "- Singer, J. David, Stuart Bremer, and John Stuckey. (1972). \"Capability Distribution, Uncertainty, and Major Power War, 1820-1965.\" in Bruce Russett (ed) Peace, War, and Numbers, Beverly Hills: Sage, 19-48. Version 5. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
